{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee53ad89-35b6-4e6a-9438-521964ed132c",
   "metadata": {},
   "source": [
    "# Lambda Athena Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db89e935-7111-4b1f-8fc8-451cde515379",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84b2458-b24a-4488-9e2d-2a5d81407e4d",
   "metadata": {},
   "source": [
    "In this lesson, let's work with a lambda function that will use Athena to query S3 when a csv file is uploaded to the relevant bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc9ff0f-85f9-4c40-a128-7ca73ec50489",
   "metadata": {},
   "source": [
    "<img src=\"./athena-workflow.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70af15-361a-4872-af91-486ae7cbc748",
   "metadata": {},
   "source": [
    "### Where we left off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e929f642-7b22-4682-be36-a8bb1e730eba",
   "metadata": {},
   "source": [
    "It probably makes sense to use some of our existing work in building our data pipeline.\n",
    "\n",
    "For example, remember in our [S3 to Athena](https://colab.research.google.com/github/data-engineering-jigsaw/s3-to-athena/blob/main/lesson/index.ipynb) lesson, that we already set up a glue crawler that would crawl our specified s3 bucket, and set up an IAM role so that glue had access to this s3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07112dbe-380b-4458-8a26-0fc2c67e2987",
   "metadata": {},
   "source": [
    "<img src=\"./crawler.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd48d53-1534-4a1d-8ad3-a72cc8165469",
   "metadata": {},
   "source": [
    "Let's begin by making sure our Athena connection to s3 still works.\n",
    "\n",
    "In our code [located here](https://github.com/data-engineering-jigsaw/lambda-athena-lab.git), if you open the `console.py` file, with some setup, you should be able to run the code at the bottom of the file.\n",
    "\n",
    "```python\n",
    "lambda_handler({}, {})\n",
    "```\n",
    "\n",
    "> **To do so**: you'll have to change the `output_bucket_name` to the bucket where you store the query results, and the `db_name`.  Then, when executing the query below, you'll have to change `jigsawtexasquery` to the bucket where your input data is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0e9bb-0773-4556-a477-0aebfc8332bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Noticing our updates\n",
    "\n",
    "Before moving on, let's just note a couple of changes that we made in the code since our last codebase.\n",
    "\n",
    "The main change is just that we wrapped our code in a function named `lambda_handler`, that takes arguments of `event` and `context`.\n",
    "\n",
    "We also made some changes that will help us with logging.  For example, at the top of the function, we print out when our function was called.  And we also print out the `results_df`.  \n",
    "\n",
    "Finally, instead of just returning the resulting dataframe, we convert the dataframe to a list of dictionaries, as lambda requires us to return a datatype that it can convert to JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0ef4c-cdba-4b00-ba0b-39561ca82073",
   "metadata": {},
   "source": [
    "* Changing the query\n",
    "\n",
    "Ok, now let's change the query a little.  For example, let's say that we only want to retrieve the records where the `total_receipts` is over 5000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9428ef57-1b7c-4a2e-8186-5a5c149294bb",
   "metadata": {},
   "source": [
    "```python\n",
    "query = \"SELECT * FROM jigsawtexasquery where total_receipts > 5000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27547de-66ec-4ec9-b10b-336009179b67",
   "metadata": {},
   "source": [
    "### Adding our lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabd357-9fdc-43dd-a9f5-4733c3cb9a67",
   "metadata": {},
   "source": [
    "Now we want to do the following. We want to set up a lambda function, so that when we drag and drop a csv file into our query bucket, the lambda function will automatically run our athena query which will place the records with `total_receipts` over 5000 into our output bucket.\n",
    "\n",
    "Essentially, we'll need to set up the lambda function to call our athena code when an S3 file is uploadded to our bucket.\n",
    "\n",
    "To do this:\n",
    "\n",
    "1. Go through the steps of creating a lambda function that has access to S3.  Look at the [following documentation](https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html) for reference.\n",
    "\n",
    "2. Then just make sure the s3 trigger is set up properly.  So upload something to the s3 relevant bucket, and use cloudwatch to check the logs and confirm that our lambda function was invoked.\n",
    "\n",
    "3. Then upload the code that queries athena to the lambda function.  And make sure it works by triggering the lambda function by sending a test event to the function.  \n",
    "\n",
    "> This will break."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441668ab-86c7-4522-8c56-87b1e8ab79f8",
   "metadata": {},
   "source": [
    "There are a couple of errors you may run into at this point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7834912-639a-4cbe-81ee-7d46964db8fd",
   "metadata": {},
   "source": [
    "* Error: `No module named lambda function`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b944089-316f-4ef7-9a12-815712cfa254",
   "metadata": {},
   "source": [
    "<img src=\"./no-module-error.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c226d65-af4d-482b-892d-0bf22becb327",
   "metadata": {},
   "source": [
    "If you changed the name of the file `lambda_function` to `console.py`, then lambda will throw an error.  This because it's looking for a file `lambda_function`, and a function inside of it called `lambda_handler` -- if it doesn't find both of those, it throws an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9939348-5adc-4950-ba62-89aee13cbe7c",
   "metadata": {},
   "source": [
    "If you scroll down in the `code` panel, you can see where this is configured, under `Runtime settings`, where the Handler is specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118fd09-0546-4618-a9a5-b47e6a99f61f",
   "metadata": {},
   "source": [
    "<img src=\"./runtime-settings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c512096-0c4b-4dc2-a13f-a9d7cee22e2d",
   "metadata": {},
   "source": [
    "So make sure you have a file called `lambda_function` and that the file has a function called `lambda_handler` that you expect to be invoked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02731d63-c217-425c-8c8c-a4030436ad3c",
   "metadata": {},
   "source": [
    "* Another error: `No module named pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc24b7-1d8d-4597-9d29-f96c5f9b146b",
   "metadata": {},
   "source": [
    "<img src=\"./no-module-pandas.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a92d8b-e968-4a47-9e2a-fb707faf63c5",
   "metadata": {},
   "source": [
    "Even if this is set up properly, you'll run into another error explaining that pandas is not installed.  The issue here is that the environment our lambda is running on does not have pandas.  \n",
    "To have pandas installed in this environment we add a layer to our lambda environment.  There's an [excellent tutorial](linkedin.com/pulse/add-external-python-libraries-aws-lambda-using-layers-gabe-olokun/) explaining how to install a custom layer - so that we can add whatever libraries we want to our environment.  However, in this case, a prebuilt AWS layer will work just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ccf55-26e4-41d8-80a1-1c2ff3f3f25c",
   "metadata": {},
   "source": [
    "To add one of AWS's prebuilt layers, from the lambda console, go to the `code` tab (the first one), and scroll down.  There, you'll see an option to `Add a layer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65e5ff-960e-4ddd-b46a-8d14381d2105",
   "metadata": {},
   "source": [
    "<img src=\"./add-layer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbe88e-1d0f-40c2-a615-53eacd1a4a34",
   "metadata": {},
   "source": [
    "So click on `Add a layer`, and keep `Aws layers` selected, and from there select the `AWSSDKPandas-Python310` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64394954-1dcd-4cfa-9a22-9e921eec7af8",
   "metadata": {},
   "source": [
    "<img src=\"./pandas-layer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdf249-d400-4558-96f1-3a9c77f4a8b4",
   "metadata": {},
   "source": [
    "Set the version to the most recent  available version, and then click `Add`.  If you look at the panel for your lambda function, you should see the number of layers updated to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d15c31-8bfc-43c1-bc8c-fe3a8b158886",
   "metadata": {},
   "source": [
    "<img src=\"./lambda-layer.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190ea12-ad4b-4420-a4cd-a951575411fc",
   "metadata": {},
   "source": [
    "Ok, so try to run the test again.  It probably won't work, but we're getting closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd681988-86b1-4c50-8897-ea63f2e908e1",
   "metadata": {},
   "source": [
    "* Another error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb4cc5b-482a-4d59-bc79-f8564e332cbe",
   "metadata": {},
   "source": [
    "<img src=\"./start-query-error.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349896f-6c3d-4856-a986-e83c11505789",
   "metadata": {},
   "source": [
    "This time, it looks like our lambda function does not have the correct permission to execute athena.  If we look at this [blog post](https://simplemaps.com/resources/athena-over-url), we can get a sense of how to fix this.  We likely need to modify the permissions listed there, to include both StartQueryExecution and GetQueryExecution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87135e98-e486-4383-b241-cf19cec12d63",
   "metadata": {},
   "source": [
    "Ok, so let's add permission to Athena by going to our lambda function's configuration, and then click on the role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2aaf9-ff8e-419b-a725-4e1dc4ade6e6",
   "metadata": {},
   "source": [
    "<img src=\"./config-role.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a36121-2604-44c7-8389-9b16a65f3e25",
   "metadata": {},
   "source": [
    "From there, add a policy that gives access to athena, glue, and the s3 input and output buckets (if permission is not already there for them), and cloudwatch (if this permission is not already included)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38161e-5c69-450b-bb43-d7aac95f03cc",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"athena:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::jigsawtexasresults/*\",\n",
    "                \"arn:aws:s3:::jigsawtexasresults\",\n",
    "                \"arn:aws:s3:::jigsawtexasquery\",\n",
    "                \"arn:aws:s3:::jigsawtexasquery/*\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"glue:*\",\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"logs:PutLogEvents\",\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:logs:*:*:*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a6e7d-ba15-4154-8673-aedcc55b629c",
   "metadata": {},
   "source": [
    "* If all else fails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e2855-2d0f-48d2-8a51-e0295263ebdf",
   "metadata": {},
   "source": [
    "If you keep getting permission errors, you can just give administrator access to the lambda function and go from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6573d74-3b6d-4717-a16a-812f5ff16a66",
   "metadata": {},
   "source": [
    "<img src=\"./admin-access.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0bc29-f6e8-4a8d-8bf4-6372d53718d8",
   "metadata": {},
   "source": [
    "* (Another error maybe) a timeout error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d58843-4b85-4f55-a281-74143d263504",
   "metadata": {},
   "source": [
    "<img src=\"./timeout-error.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203cbd47-3c6f-4688-a556-4a667d5e13e3",
   "metadata": {},
   "source": [
    "At this point you may get a timeout error.  [Google this error](https://stackoverflow.com/questions/62948910/aws-lambda-errormessage-task-timed-out-after-3-00-seconds) and see how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aae622-e1c2-42cf-b462-dffbec1be232",
   "metadata": {},
   "source": [
    "If you are able to get the test event to work move onto the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c56ff1-10e3-4892-bbdd-8e239acc92be",
   "metadata": {},
   "source": [
    "4. Upload the csv file that is provided to the s3 bucket and confirm that the query results are in the results bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f61d88-f274-4540-a93b-1c45e2de2378",
   "metadata": {},
   "source": [
    "### Triggering the lambda from boto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dc87c7-51b9-4fdf-a94a-eab1a6595b38",
   "metadata": {},
   "source": [
    "So now that we uploaded a file, and seen our lambda function called, which executes athena.  The next step is to use boto to make a request.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4344d7-277a-40bd-805d-af94828ef750",
   "metadata": {},
   "source": [
    "<img src=\"./updated-pipeline.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48066c-1190-4fc8-96c0-731119bea8a1",
   "metadata": {},
   "source": [
    "This should make a request to the external api, and upload a new object to the query bucket.  Then the new object in the query bucket should call the lambda function, which will have athena query the bucket and place the matching results in our output bucket.\n",
    "\n",
    "We can kick this off by navigating to the `extract_load` folder.  And from there, you can update the `query_bucket_name` to be your query bucket.  And then call the `upload_console.py` file.  \n",
    "\n",
    "Confirm this occurred with the cloudwatch logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7171677-ef29-40c9-b6cf-2afe95016d14",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb722ce-b17f-411f-86cf-4d4365366f83",
   "metadata": {},
   "source": [
    "In this lesson, we set up a pipeline so that when we upload a file to S3, our lambda function triggers athena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d981e852-07c1-46e6-912d-46bb8d0950ab",
   "metadata": {},
   "source": [
    "### What's next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861590a-ef29-4739-8b32-e11ababf4614",
   "metadata": {},
   "source": [
    "One thing to improve from the above is that when we make a request to the API, we then query our entire bucket to find the selected queries.  \n",
    "\n",
    "It would be better if we only select the recently updated queries.  One way to accomplish this would be, when querying the API to store the results in a dated folder (for example receipts/may_31_2023).  Glue will turn these folders into a different column values for a column called partition.  And in our query we can select for those records in the current dates folder that also meet any additional criteria (eg. having total receipts over 5000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fcf57d-d47d-4c15-85c0-b95df8e7d383",
   "metadata": {},
   "source": [
    "```python\n",
    "from datetime import date\n",
    "today = date.today().strftime(\"%b_%d_%Y\")\n",
    "query_results(f\"SELECT * FROM jigsawtexasquery where total_receipts > 5000 and partition = {today}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a0cb1-56ab-4e69-983a-afcb5745d5df",
   "metadata": {},
   "source": [
    "If you're feeling frisky try to get our code to only search through the recently updated files.  And from there, you may want to look into [adding a custom layer](https://www.linkedin.com/pulse/add-external-python-libraries-aws-lambda-using-layers-gabe-olokun/) instead of using our pre-built one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2475dc1-50fe-4133-81af-5176d3a4bd2a",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Adding a custom layer](https://www.linkedin.com/pulse/add-external-python-libraries-aws-lambda-using-layers-gabe-olokun/)\n",
    "\n",
    "[Athena Permissions](https://simplemaps.com/resources/athena-over-url)\n",
    "\n",
    "[Another Athena Permissions Example](https://github.com/ShivakumarRavi/AWS-fetch_athena_data_from_lambda/tree/main)\n",
    "\n",
    "[AWS EFS vs EBS](https://stackoverflow.com/questions/29575877/aws-efs-vs-ebs-vs-s3-differences-when-to-use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
